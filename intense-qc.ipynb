{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9380dac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Intense QC for DAFNI workflow\n",
    "# Amy Green, Robin Wardle\n",
    "# July 2023\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "# Python libraries\n",
    "###############################################################################\n",
    "from os.path import join, exists\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a205edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Constants\n",
    "###############################################################################\n",
    "QC_SUCCESS_FILENAME = \"success\"\n",
    "QC_LOG_FILENAME = \"intense-qc.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3623cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Paths\n",
    "###############################################################################\n",
    "\n",
    "# Output paths to save files\n",
    "platform = os.getenv(\"INTENSE_QC_ENV\")\n",
    "if platform==\"docker\":\n",
    "    data_path = os.getenv(\"DATA_PATH\", \"/data\")\n",
    "else:\n",
    "    data_path = os.getenv(\"DATA_PATH\", \"./data\")\n",
    "input_path = os.path.join(data_path, \"inputs\")\n",
    "output_path = os.path.join(data_path, \"outputs\")\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Reset the success and log files\n",
    "if os.path.isfile(os.path.join(output_path, QC_SUCCESS_FILENAME)):\n",
    "    os.remove(os.path.join(output_path, QC_SUCCESS_FILENAME))\n",
    "if os.path.isfile(os.path.join(output_path, QC_LOG_FILENAME)):\n",
    "    os.remove(os.path.join(output_path, QC_LOG_FILENAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0e2c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:Logger initialised\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:data_path = ./data\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n",
      "INFO:intense-qc:output_path = ./data/outputs\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Logging\n",
    "###############################################################################\n",
    "# Configure logging\n",
    "logging.basicConfig()\n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "# Logging instance\n",
    "#logger = logging.getLogger(pathlib.PurePath(__file__).name)\n",
    "logger = logging.getLogger(\"intense-qc\")\n",
    "logger.propagate = False\n",
    "\n",
    "# Console messaging\n",
    "console_formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n",
    "console_handler = logging.StreamHandler(stream=sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(console_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# File logging\n",
    "file_formatter = logging.Formatter('%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "file_handler = logging.FileHandler(output_path / pathlib.Path(QC_LOG_FILENAME))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(file_formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "logger.info(\"Logger initialised\")\n",
    "\n",
    "# Some additional logging info\n",
    "logger.info(\"data_path = {}\".format(data_path))\n",
    "logger.info(\"output_path = {}\".format(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c883ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Other Parameters\n",
    "#   Remember that parameters are passed as environment variables,\n",
    "#   i.e. they are strings and will need to be converted\n",
    "###############################################################################\n",
    "\n",
    "# Dates for files\n",
    "start_date = os.getenv(\"RUN_START_DATE\", \"2023-06-20\")\n",
    "end_date = os.getenv(\"RUN_END_DATE\", \"2023-06-30\")\n",
    "\n",
    "# Bounding box for data\n",
    "# e_l, n_l, e_u, n_u = [355000, 534000, 440000, 609000]\n",
    "try:\n",
    "    e_l = int(os.getenv(\"BB_E_L\", \"355000\"))\n",
    "    n_l = int(os.getenv(\"BB_N_L\", \"534000\"))\n",
    "    e_u = int(os.getenv(\"BB_E_U\", \"440000\"))\n",
    "    n_u = int(os.getenv(\"BB_N_U\", \"609000\"))\n",
    "    bbox = [e_l, e_u, n_l, n_u]\n",
    "except (TypeError, ValueError, Exception) as e:\n",
    "    logger.error(\"Error converting environmental parameters: {}\".format(e))\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a43b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Quality control test data library path\n",
    "# TODO: This lib needs to be incorporated as a sub-repo, not as copied files\n",
    "###############################################################################\n",
    "static_data_path = join(r\"./\", \"static\")\n",
    "intense_path = join(static_data_path, \"intense-qc\")\n",
    "etccdi_data_path = join(join(intense_path, \"tests\"), \"etccdi_data\")\n",
    "\n",
    "# Adds intense folder location to path so can run correct version\n",
    "os.sys.path.append(intense_path)\n",
    "from intense import gauge, qc, utils\n",
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "58cf5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gauge_flags(station_id, data, loc):\n",
    "    '''\n",
    "    Function to apply wokring Intense QC hourly checks on gauge data. \n",
    "    Args: data is the gauge data (pd.Series), loc is the gauge location (eastings, northings)\n",
    "    Returns: flags as a dictionary {gauge flagged, years flagged, obs flagged}\n",
    "    '''\n",
    "    eastings, northings = loc\n",
    "    # convert coordinates to lat/lon\n",
    "    transformer = Transformer.from_crs(\"epsg:27700\", \"epsg:4326\")\n",
    "    latitude, longitude = transformer.transform(eastings, northings)\n",
    "    \n",
    "    # create gauge object\n",
    "    rain_gauge = gauge.Gauge(\n",
    "        station_id,\n",
    "        path_to_original_data=\"\",\n",
    "        latitude=latitude,\n",
    "        longitude=longitude,\n",
    "        original_timestep=\"15min\",\n",
    "        original_units=\"mm/h\",\n",
    "        new_units=\"mm/h\",\n",
    "        new_timestep=\"1h\",\n",
    "        data=data\n",
    "    )\n",
    "    rain_gauge.get_info()\n",
    "\n",
    "    # create qc object \n",
    "    test = qc.Qc(\n",
    "        gauge=rain_gauge,\n",
    "        etccdi_data_folder=etccdi_data_path\n",
    "    )\n",
    "\n",
    "    # checks that don't work on existing data\n",
    "    \"\"\"\n",
    "    test.check_percentiles()\n",
    "    test.check_k_largest()\n",
    "    test.check_intermittency()\n",
    "    test.change_in_min_val_check() \n",
    "    test.cwd_check() # missing function in utils file???\n",
    "    test.change_in_min_val_check() # Change in minimum value check, homogeneity check to see if the resolution of the data has changed. Change flag, flag years\n",
    "    test.find_neighbours(\"hourly\") # frequency: must be either hourly, daily or monthly, Names or names and paths of neighbouring stations\n",
    "    # conditions are: must be within 50km, at least 3 years overlap, select the closest 10, don't have three years of data\n",
    "    # check_hourly_neighbours(), check_daily_neighbours(), check_monthly_neighbours()\n",
    "    test.get_flags() # runs all checks, fails at find_neighbours()\n",
    "    \"\"\"\n",
    "\n",
    "    ### run checks ###\n",
    "\n",
    "    flagged_sdii = 0\n",
    "    sdii_thresh = 100 # just arbitrary atm\n",
    "    if any(np.array(test.get_sdii()) > sdii_thresh): # Simple precipitation intensity index, SDII from ETCCDI and from gauge values (sdii_gridded, sdii_gauge), not sure how to use this\n",
    "        flagged_sdii = 1\n",
    "    \n",
    "    # Flag data if any of these don't return 0:\n",
    "    gauge_checks = [\n",
    "        test.check_days_of_week(), # Checks if proportions of rainfall in each day is significantly different\n",
    "        test.check_break_point(), # Pettitt breakpoint check\n",
    "        flagged_sdii\n",
    "    ]\n",
    "    \n",
    "    flagged_gauge = sum(gauge_checks) != 0\n",
    "    #if flagged_gauge:\n",
    "    #    print(\"Gauge\", np.array([\"days of week\", \"break point\", \"sdii\"])[np.array(gauge_checks) == 1])\n",
    "    #    print(test.check_break_point())\n",
    "\n",
    "    # Flag individual data observations if don't return 0:\n",
    "    obs_checks = pd.DataFrame(index=rain_gauge.data.index)\n",
    "    obs_checks[\"world_record\"] = test.world_record_check_ts() # Checks if and to what degree the world record has been exceeded by each rainfall value, 4, 3, 2 or 1 if exceeded by > 1.5x, 1.33x, 1.22x or 0x respectively and 0 if not exceeded for each value\n",
    "    obs_checks[\"rx1day\"] = test.rx1day_check_ts() # Checks hourly values against maximum 1-day precipitation, Magnitudes of exceedance for each day\n",
    "    obs_checks[\"cdd\"] = test.cdd_check() # ETCCDI provide an index for maximum length of dry spell. Look for suspicious number of consecutive dry hours recorded. Consecutive Dry Days: Maximum length of dry spell, maximum number of consecutive days with RR < 1mm. Magnitudes of exceedence of the length of longest dry period\n",
    "    obs_checks[\"daily_accums\"] = test.daily_accums_check() # Check daily accumulations. Suspect daily accumulations flagged where a recorded rainfall amount at these times is preceded by 23 hours with no rain. A threshold of 2x the mean wet day amount for the corresponding month is applied to increase thechance of identifying accumulated values at the expense of genuine, moderate events\n",
    "    obs_checks[\"monthly_accums\"] = test.monthly_accums_check() # Check monthly accumulations. Flags month prior to high value\n",
    "    obs_checks[\"streaks\"] = test.streaks_check() # Streaks: This is where you see the same value repeated in a run. Currently this records streaks of 2hrs in a row or more over 2 x Monthly mean rainfall. It is considered to be unlikely that you would see even 2 consecutive large rainfall amounts. For this code I have substituted the monthly mean rainfall for SDII as I want the thresholds to be independent of the rainfall time series as the global dataset is of highly variable quality.\n",
    "    flagged_obs = obs_checks.index[obs_checks.sum(1) > 0]\n",
    "\n",
    "    # Flags each individual year:\n",
    "    year_flags = np.array([\n",
    "        test.r99ptot_check_annual(), # Check against R99pTOT: R99pTOT. Annual total PRCP when RR > 99p. Magnitudes of exceedance for yearly 99th percentiles\n",
    "        test.prcptot_check_annual() # check against annual total: PRCPTOT. Annual total precipitation in wet days. Magnitudes of exceedance for yearly totals\n",
    "    ])\n",
    "\n",
    "    flagged_years = np.arange(rain_gauge.data.index.min().year, rain_gauge.data.index.max().year + 1)[year_flags.sum(0) < 0]\n",
    "\n",
    "    return({\"gauge\" : flagged_gauge, \"years\" : flagged_years, \"obs\" : flagged_obs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "972b73f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'station_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166624/3363014807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meastings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorthings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_gauge_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgauge_1hr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Only include gauge data that is not flagged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_166624/1199399718.py\u001b[0m in \u001b[0;36mget_gauge_flags\u001b[0;34m(station_id, data, loc)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnew_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mm/h\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mnew_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1h\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     \u001b[0mrain_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'station_id'"
     ]
    }
   ],
   "source": [
    "gauge_sources = [\"EA\", \"UO\", \"CS\", \"NGIF\"]\n",
    "\n",
    "for source in gauge_sources:\n",
    "\n",
    "    source_input_filepath = join(input_path, source)\n",
    "    source_output_filepath = join(output_path, source)\n",
    "\n",
    "    os.makedirs(source_output_filepath, exist_ok=True)\n",
    "\n",
    "    for f in os.listdir(source_input_filepath):\n",
    "\n",
    "        if f.endswith(\".csv\"):\n",
    "            gauge_15min = pd.read_csv(join(source_input_filepath, f), index_col=0)\n",
    "            gauge_15min.index = pd.to_datetime(gauge_15min.index, utc=True)\n",
    "\n",
    "            gauge_1hr = gauge_15min.resample(\"1h\").mean()\n",
    "            station_id, eastings, northings = f.split(\".\")[0].split(\"_\")\n",
    "    \n",
    "            loc = (eastings, northings)\n",
    "            flags = get_gauge_flags(station_id, gauge_1hr.dropna(), loc)\n",
    "    \n",
    "            # Only include gauge data that is not flagged\n",
    "            if not flags[\"gauge\"]:\n",
    "                # currenly ignoring year flag as not enough data\n",
    "\n",
    "                # remove flagged observationns\n",
    "                if len(flags[\"obs\"]) > 0:\n",
    "                    cond = [idx in flags[\"obs\"] for idx in gauge_15min.index.round(\"1h\")]\n",
    "                    gauge_15min.loc[cond] = np.nan\n",
    "\n",
    "                gauge_15min.tz_localize('UTC').to_csv(join(source_output_filepath, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Conclusion\n",
    "###############################################################################\n",
    "logger.info(\"Completed successfully\")\n",
    "os.system(\"cd \" + output_path + \"; touch success\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
